{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26e9ec12-815a-4073-8891-cb6888c41cdb",
   "metadata": {},
   "source": [
    "# Search\n",
    "\n",
    "In this homework, you'll implement a basic search engine by defining your own Python classes. A **search engine** is an algorithm that takes a query and retrieves the most relevant documents for that query. In order to identify the most relevant documents, our search engine will use **term frequency–inverse document frequency** ([tf–idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)), an information statistic for determining the relevance of a term to each document from a corpus consisting of many documents.\n",
    "\n",
    "The **tf–idf statistic** is a product of two values: term frequency and inverse document frequency. **Term frequency** computes the number of times that a term appears in a **document** (such as a single Wikipedia page). If we were to use only the term frequency in determining the relevance of a term to each document, then our search result might not be helpful since most documents contain many common words such as \"the\" or \"a\". In order to downweight these common terms, the **document frequency** computes the number of times that a term appears across the **corpus** of all documents.\n",
    "\n",
    "You will need the `doggos` and `small_wiki` directories to be copied to your work area in the same folder. You do not need to submit these folders but should submit this notebook file after `Run All` and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed02adc-9d59-4e97-bf14-ef02865ea17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 1.0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# You must pass all mypy strict syntax checks\n",
    "\n",
    "%pip install -q nb-mypy\n",
    "%reload_ext nb_mypy\n",
    "%nb_mypy mypy-options --strict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96ac5cd5-dd9f-4c73-81f2-e2beb91c8952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q pytest ipytest\n",
    "\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "\n",
    "import pytest\n",
    "import ipytest\n",
    "ipytest.autoconfig()\n",
    "\n",
    "\n",
    "def clean(token: str, pattern: re.Pattern[str] = re.compile(r\"\\W+\")) -> str:\n",
    "    \"\"\"\n",
    "    Returns all the characters in the token lowercased and without matches to the given pattern.\n",
    "\n",
    "    >>> clean(\"Hello!\")\n",
    "    'hello'\n",
    "    \"\"\"\n",
    "    return pattern.sub(\"\", token.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b461b246-7c8f-40cd-96ab-8dafa6559526",
   "metadata": {},
   "source": [
    "## Outside Sources\n",
    "\n",
    "Update the following Markdown cell to include your name and list your outside sources. Submitted work should be consistent with the curriculum and your sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa59fea8-bfd9-45b7-806e-9bf664a4c958",
   "metadata": {},
   "source": [
    "**Name**: Aadhya Goyal\n",
    "\n",
    "1. https://www.programiz.com/python-programming/methods/string/count\n",
    "2. https://www.w3schools.com/python/python_sets.asp\n",
    "3. getting all the values in a dictionary: https://www.geeksforgeeks.org/python/how-can-to-get-list-of-values-from-dictionary-in-python/\n",
    "4. Python sets: https://www.w3schools.com/python/python_sets.asp\n",
    "5. formatting string: https://www.w3schools.com/python/python_string_formatting.asp \n",
    "6. https://stackoverflow.com/questions/59390797/how-to-type-as-a-string-in-python \n",
    "7. merging without duplicates: https://www.geeksforgeeks.org/python/python-merge-two-lists-without-duplicates/ \n",
    "8. https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value \n",
    "9. dictionary comphrehensions: https://www.geeksforgeeks.org/python/python-dictionary-comprehension/ \n",
    "10. default dict: https://docs.python.org/3/library/collections.html#collections.defaultdict \n",
    "11. https://www.w3schools.com/python/python_sets_join.asp \n",
    "12. https://www.w3schools.com/python/python_operators_bitwise.asp \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7e58631-86a0-4dde-a43c-e8bc0cd9e6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
      "platform linux -- Python 3.12.1, pytest-9.0.1, pluggy-1.6.0\n",
      "rootdir: /workspaces/cse163-nchsAadhyaGoyal/homework\n",
      "plugins: anyio-4.11.0\n",
      "collected 3 items\n",
      "\n",
      "t_4acc4ba92da4482aa9d6777bede306cd.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                    [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -v\n",
    "\n",
    "class Document:\n",
    "\n",
    "    \"\"\"\n",
    "    Represents a single text document, storing its tokenized words\n",
    "    and providing methods for term frequency and token access.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, path: str) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize a Document object by reading the file at `path`,\n",
    "        tokenizing its contents, cleaning each token, and storing\n",
    "        token frequencies in a dictionary.\n",
    "\n",
    "        Args:\n",
    "            path (str): Path to the text file to read.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        self.path = path\n",
    "        self.words: dict[str, int] = {}\n",
    "\n",
    "\n",
    "        with open (path) as f:\n",
    "            fileString = f.read()\n",
    "            # file is a string now\n",
    "\n",
    "            # First, we split the string\n",
    "            tokens = fileString.split()\n",
    "\n",
    "            # now that we have a list of all the words, its time to compile a dictionary\n",
    "            # in this for loop, we iterate through each word in the list of words\n",
    "            for token in tokens:\n",
    "                # first, before doing anything with the token, we need to clean it\n",
    "                token = clean(token)\n",
    "                \n",
    "                # check if the word already exists. \n",
    "                if(token in self.words):\n",
    "                    # this means that the token exists\n",
    "                    # update frequency without creating a new word\n",
    "                    self.words[token] += 1\n",
    "                else :\n",
    "                    # the token doesn't exist\n",
    "                    # create token and set frequency to be 1\n",
    "                    self.words[token] = 1\n",
    "            \n",
    "\n",
    "    def term_frequency(self, term: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute the term frequency (TF) of a given term in this document.\n",
    "        TF = count of the term / total number of tokens.\n",
    "\n",
    "        Args:\n",
    "            term (str): The term to calculate the frequency for.\n",
    "\n",
    "        Returns:\n",
    "            float: Term frequency of `term`. Returns 0.0 if the term is\n",
    "                   not present or the document is empty.\n",
    "        \"\"\"\n",
    "        # print(\"time\", time.time() - self.start)\n",
    "        if(len(self.words) == 0 or self.words.get(clean(term), 0) == 0 or (sum(list(self.words.values()))) == 0):\n",
    "            return 0.0\n",
    "        return (self.words[clean(term)]/(sum(list(self.words.values()))))\n",
    "    \n",
    "    def get_words(self) -> set[str]:\n",
    "        \"\"\"\n",
    "        Get all unique cleaned words in the document.\n",
    "\n",
    "        Returns:\n",
    "            set[str]: A set of unique token strings in this document.\n",
    "        \"\"\"\n",
    "        return set(self.words.keys())\n",
    "\n",
    "    def get_path(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the file path of this document.\n",
    "\n",
    "        Returns:\n",
    "            str: The path to the document file.\n",
    "        \"\"\"\n",
    "        return self.path\n",
    "\n",
    "class TestDocument:\n",
    "    doc1 = Document(\"doggos/doc1.txt\")\n",
    "    euro = Document(\"small_wiki/Euro - Wikipedia.html\")\n",
    "    testCase3 = Document(\"customTestFiles/sallySells.txt\")\n",
    "    testCase4 = Document(\"customTestFiles/quote1.txt\")\n",
    "    testCase5 = Document(\"customTestFiles/Empty.txt\")\n",
    "    testCase6 = Document(\"customTestFiles/codingQuote1.txt\")\n",
    "\n",
    "    def test_term_frequency(self) -> None:\n",
    "        assert self.doc1.term_frequency(\"dogs\") == pytest.approx(1 / 5)\n",
    "        assert self.euro.term_frequency(\"Euro\") == pytest.approx(0.0086340569495348)\n",
    "        assert self.testCase3.term_frequency(\"Sally\") == pytest.approx(0.125)\n",
    "        assert self.testCase4.term_frequency(\"you\") == pytest.approx(0.1428571429)\n",
    "        assert self.testCase5.term_frequency(\"Hello\") == pytest.approx(0)\n",
    "        assert self.testCase6.term_frequency(\"...\") == pytest.approx(0)\n",
    "        \n",
    "\n",
    "    def test_get_words(self) -> None:\n",
    "        assert self.doc1.get_words() == set(\"dogs are the greatest pets\".split())\n",
    "        assert set(w for w in self.euro.get_words() if len(w) == 1) == set([\n",
    "            *\"0123456789acefghijklmnopqrstuvxyz\".lower() # All one-letter words in Euro\n",
    "        ])\n",
    "\n",
    "        assert self.testCase3.get_words() == set(\"sally sea shells but if by seashore then where are the sells\".split())\n",
    "        assert self.testCase5.get_words() == set()\n",
    "\n",
    "    def test_get_path(self) -> None:\n",
    "        assert self.doc1.get_path() == \"doggos/doc1.txt\"\n",
    "        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "793ae51f-db9c-4859-97a2-a58267ec4627",
   "metadata": {},
   "source": [
    "## Task: `SearchEngine`\n",
    "\n",
    "Write and test a `SearchEngine` class in the code cell below that represents a corpus of `Document` objects and includes methods to compute the tf–idf statistic between a given query and every document in the corpus. The `SearchEngine` begins by constructing an **inverted index** that associates each term in the corpus to the list of `Document` objects that contain the term.\n",
    "\n",
    "To iterate over all the files in a directory, call `os.listdir` to list all the file names and join the directory to the filename with `os.path.join`. The example below will print only the `.txt` files in the `doggos` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9893aeb0-1048-4527-851f-78b77df8c0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doggos/doc1.txt\n",
      "doggos/doc3.txt\n",
      "doggos/doc2.txt\n"
     ]
    }
   ],
   "source": [
    "path = \"doggos\"\n",
    "extension = \".txt\"\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(extension):\n",
    "        print(os.path.join(path, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b7ff12-8c72-49bf-9e5f-45024044340f",
   "metadata": {},
   "source": [
    "The `SearchEngine` class should include:\n",
    "\n",
    "1. An initializer that takes a `str` path to a directory such as `\"small_wiki\"` and a `str` file extension and constructs an inverted index from the files in the specified directory matching the given extension. By default, the extension should be `\".txt\"`. Assume the string represents a valid directory, and that the directory contains only valid files. Do not recreate any behavior that is already done in the `Document` class—call the `get_words()` method! Create at most one `Document` per file.\n",
    "\n",
    "1. A method `_calculate_idf` that takes a `str` term and returns the inverse document frequency of that term. If the term is not in the corpus, return 0. Inverse document frequency is defined by calling `math.log` on *the total number of documents in the corpus* divided by *the number of documents containing the given term*.\n",
    "\n",
    "1. A method `__repr__` that returns a string representation of this search engine in the format `SearchEngine('{path}')` (with literal single quotes in the output) where `{path}` is the path to the directory specified in the initializer.\n",
    "\n",
    "1. A method `search` that takes a `str` **query** consisting of one or more terms and returns a `list` of relevant document paths that match at least one of the cleaned terms sorted by descending tf–idf statistic: the product of the term frequency and inverse document frequency. If there are no matching documents, return an empty list.\n",
    "\n",
    "**For each of the 3 methods (excluding the initializer) in the `SearchEngine` class, write a testing function that contains at least 3 `pytest`-style assertions based on your own testing corpus and written as expressions like 1 / 5 that show your thinking**. Test cases for `SearchEngine.__repr__` may use the given corpuses `doggos` and `small_wiki` in addition to your corpus. Documentation strings are optional for testing functions.\n",
    "\n",
    "- The type checker can be too picky about the `sorted` `key` parameter: a type error here can be safely ignored if the code works as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac19da1b-da24-46da-add8-7065e09b5789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to initialize: 0.0001690387725830078\n",
      "Time taken to initialize: 1.146643877029419\n",
      "Time taken to initialize: 0.0002892017364501953\n",
      "Time taken to search: 0.006936550140380859\n",
      "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
      "platform linux -- Python 3.12.1, pytest-9.0.1, pluggy-1.6.0\n",
      "rootdir: /workspaces/cse163-nchsAadhyaGoyal/homework\n",
      "plugins: anyio-4.11.0\n",
      "collected 1 item\n",
      "\n",
      "t_4acc4ba92da4482aa9d6777bede306cd.py \u001b[32m.\u001b[0m\u001b[32m                                                      [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -v\n",
    "from collections import defaultdict\n",
    "import time\n",
    "# NOTE: ALL PRINTED TIMES ARE IN SECONDS. 1.0 IS ONE SECOND\n",
    "class SearchEngine:\n",
    "\n",
    "    \"\"\"\n",
    "    A search engine that indexes a directory of documents and allows\n",
    "    TF-IDF-based search queries.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, path: str, fileExtension: str = \".txt\") -> None:\n",
    "        startTime = time.time()\n",
    "        \"\"\"\n",
    "        Initialize the SearchEngine by loading documents from a directory\n",
    "        and building an inverted index.\n",
    "\n",
    "        Args:\n",
    "            path (str): Directory path containing document files.\n",
    "            fileExtension (str, optional): File extension to filter documents. Defaults to \".txt\".\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        self.path = path #the path of the file\n",
    "        self.documents = set() #all the documents that have that ending\n",
    "\n",
    "        self.IDI: dict[str, set[Document]] = defaultdict(set) #a dictionary of the words and the document objects where they can be found\n",
    "        # ^^^^set avoids duplicates, creates empty set if key doens't exist \n",
    "        \n",
    "        for filename in os.listdir(path):\n",
    "            # this filters only for the specific file extension\n",
    "            if filename.endswith(fileExtension):\n",
    "                # then we create a new document object and add it to the set\n",
    "                self.documents.add(Document(os.path.join(path, filename)))\n",
    "        \n",
    "        # now that all the files have been stored into documents, we need to construct the inverted index\n",
    "        \n",
    "\n",
    "        # lets start by putting all the words and docs into a dictionary\n",
    "        docsAndWords = {doc: doc.get_words() for doc in self.documents}\n",
    "\n",
    "        \n",
    "        for doc, words in docsAndWords.items():\n",
    "            for word in set(words):  # remove duplicates per doc\n",
    "                self.IDI[word].add(doc)  # fast append + no duplicates\n",
    "\n",
    "        print(\"Time taken to initialize:\", time.time()-startTime)\n",
    "\n",
    "        \n",
    "\n",
    "    # this calculates the IDF for the term\n",
    "    def _calculate_idf(self, term: str) -> float:\n",
    "        # startTime = time.time()\n",
    "        \"\"\"\n",
    "        Compute the inverse document frequency (IDF) for a given term.\n",
    "\n",
    "        Args:\n",
    "            term (str): The term to compute IDF for.\n",
    "\n",
    "        Returns:\n",
    "            float: IDF value. Returns 0 if term is not found or there are no documents.\n",
    "        \"\"\"\n",
    "        if((len(self.IDI[term]) == 0) or (len(self.documents) == 0)):\n",
    "            return 0\n",
    "\n",
    "\n",
    "        output = math.log(len(self.documents)/len(self.IDI[term]))\n",
    "        # the if statement makes sure that there aren't any errors due to 0\n",
    "        \n",
    "        # print(\"Time taken to calculate idf:\", time.time()-startTime)\n",
    "        # calculate idf by log(dividing the two)    \n",
    "        return output\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "\n",
    "        \"\"\"\n",
    "        Return a string representation of the search engine.\n",
    "        \"\"\"\n",
    "        return f\"SearchEngine('{{self.path}}')\"\n",
    "    \n",
    "    def search(self, query: str) -> list[str]:\n",
    "        \n",
    "        \"\"\"\n",
    "        Search for documents matching a query using TF-IDF ranking.\n",
    "\n",
    "        Args:\n",
    "            query (str): Space-separated query words.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: List of document paths sorted by relevance (highest TF-IDF first).\n",
    "        \"\"\"\n",
    "        startTime = time.time()\n",
    "\n",
    "        # take the query and split it into a list of words\n",
    "        \n",
    "        # Get the documents for each word and then put them together into one big set of documents\n",
    "        allDocs = set()\n",
    "        for word in query.split():\n",
    "            allDocs |= self.IDI.get(word, set())\n",
    "    \n",
    "        # templist: List[Document] = []\n",
    "        # for word in searchWords:\n",
    "        #     # get the docs in the dictionary\n",
    "        #     tempList.append(self.IDI[word])\n",
    "        \n",
    "        docsDictionary = {}\n",
    "\n",
    "        # then, for each document, calculate the tf-df for each word and add it up\n",
    "        for doc in set(allDocs):\n",
    "            score = 0.0\n",
    "            for word in query.split():\n",
    "                score += doc.term_frequency(word) * self._calculate_idf(word)\n",
    "            docsDictionary[doc.get_path()] = score\n",
    "            \n",
    "        print(\"Time taken to search:\", time.time()-startTime)\n",
    "        # now we need to sort the dictionary\n",
    "        return [doc_path for doc_path, score in sorted(docsDictionary.items(), key=lambda item: item[1], reverse=True)]\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "class TestSearchEngine:\n",
    "\n",
    "    \n",
    "    doggos = SearchEngine(\"doggos\")\n",
    "    small_wiki = SearchEngine(\"small_wiki\", \".html\")\n",
    "    customTests = SearchEngine(\"customTestFiles\")\n",
    "    small_wiki.search(\"data\") #using it to test worst search runtime\n",
    "\n",
    "    def test_search(self) -> None:\n",
    "        \n",
    "\n",
    "        assert self.doggos.search(\"love\") == [\"doggos/doc3.txt\"]\n",
    "        assert self.doggos.search(\"dogs\") == [\"doggos/doc3.txt\", \"doggos/doc1.txt\"]\n",
    "        assert self.doggos.search(\"cats\") == [\"doggos/doc2.txt\"]\n",
    "        assert self.doggos.search(\"love dogs\") == [\"doggos/doc3.txt\", \"doggos/doc1.txt\"]\n",
    "        assert self.small_wiki.search(\"data\")[:10] == [\n",
    "            \"small_wiki/Internet privacy - Wikipedia.html\",\n",
    "            \"small_wiki/Machine learning - Wikipedia.html\",\n",
    "            \"small_wiki/Bloomberg L.P. - Wikipedia.html\",\n",
    "            \"small_wiki/Waze - Wikipedia.html\",\n",
    "            \"small_wiki/Digital object identifier - Wikipedia.html\",\n",
    "            \"small_wiki/Chief financial officer - Wikipedia.html\",\n",
    "            \"small_wiki/UNCF - Wikipedia.html\",\n",
    "            \"small_wiki/Jackson 5 Christmas Album - Wikipedia.html\",\n",
    "            \"small_wiki/KING-FM - Wikipedia.html\",\n",
    "            \"small_wiki/The News-Times - Wikipedia.html\",\n",
    "        ]\n",
    "        assert self.customTests.search(\"seashore\") == [\"customTestFiles/sallySells.txt\"]\n",
    "        assert self.customTests.search(\"the\") == [\"customTestFiles/quote1.txt\", \"customTestFiles/sallySells.txt\"]\n",
    "        assert self.customTests.search(\"\") == []\n",
    "        assert self.customTests.search(\"pinkiePie\") == []\n",
    "        # removed timeit in the final submission because it increased runtime by ~ one minute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63229ee-a94e-4b2d-ba34-4a87cccb2eec",
   "metadata": {},
   "source": [
    "We recommend the following iterative software development approach to implement the `search` method.\n",
    "\n",
    "1. Write code to handle queries that contain only a single term by collecting all the documents that contain the given term, computing the tf–idf statistic for each document, and returning the list of document paths sorted by descending tf–idf statistic.\n",
    "\n",
    "1. Write tests to ensure that your program works on single-term queries.\n",
    "\n",
    "1. Write code to handle queries that contain more than one term by returning all the documents that match any of the terms in the query sorted by descending tf–idf statistic. The tf–idf statistic for a document that matches more than one term is defined as the sum of its constituent single-term tf–idf statistics.\n",
    "\n",
    "1. Write tests to ensure that your program works on multi-term queries.\n",
    "\n",
    "Here's a walkthrough of the `search` function from beginning to end. Say we have a corpus in a directory called `\"doggos\"` containing 3 documents with the following contents:\n",
    "\n",
    "- `doggos/doc1.txt` with the text `Dogs are the greatest pets.`\n",
    "- `doggos/doc2.txt` with the text `Cats seem pretty okay`\n",
    "- `doggos/doc3.txt` with the text `I love dogs!`\n",
    "\n",
    "The initializer should construct the following inverted index.\n",
    "\n",
    "```python\n",
    "{\"dogs\":     [doc1, doc3],\n",
    " \"are\":      [doc1],\n",
    " \"the\":      [doc1],\n",
    " \"greatest\": [doc1],\n",
    " \"pets\":     [doc1],\n",
    " \"cats\":     [doc2],\n",
    " \"seem\":     [doc2],\n",
    " \"pretty\":   [doc2],\n",
    " \"okay\":     [doc2],\n",
    " \"i\":        [doc3],\n",
    " \"love\":     [doc3]}\n",
    "```\n",
    "\n",
    "Searching this corpus for the multi-term query `\"love dogs\"` should return a list `[\"doggos/doc3.txt\", \"doggos/doc1.txt\"]` by:\n",
    "\n",
    "1. Finding all documents that match at least one query term. The word `\"love\"` is found in `doc3` while the word `\"dogs\"` is found in `doc1` and `doc3`.\n",
    "\n",
    "1. Computing the tf–idf statistic for each matching document. For each matching document, the tf–idf statistic for a multi-word query `\"love dogs\"` is the sum of the tf–idf statistics for `\"love\"` and `\"dogs\"` individually.\n",
    "\n",
    "   1. For `doc1`, the sum of 0 + 0.081 = 0.081. The tf–idf statistic for `\"love\"` is 0 because the term is not in `doc1`.\n",
    "\n",
    "   1. For `doc3`, the sum of 0.366 + 0.135 = 0.501.\n",
    "\n",
    "1. Returning the matching document paths sorted by descending tf–idf statistic.\n",
    "\n",
    "After completing your `SearchEngine`, run the following cell to search our small Wikipedia corpus for the query \"data\". Try some other search queries too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0555e6ef-ea88-427b-957d-8cc0bfcc6e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to initialize: 1.3193671703338623\n",
      "Time taken to search: 0.005924701690673828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['small_wiki/Internet privacy - Wikipedia.html',\n",
       " 'small_wiki/Machine learning - Wikipedia.html',\n",
       " 'small_wiki/Bloomberg L.P. - Wikipedia.html',\n",
       " 'small_wiki/Waze - Wikipedia.html',\n",
       " 'small_wiki/Digital object identifier - Wikipedia.html',\n",
       " 'small_wiki/Chief financial officer - Wikipedia.html',\n",
       " 'small_wiki/UNCF - Wikipedia.html',\n",
       " 'small_wiki/Jackson 5 Christmas Album - Wikipedia.html',\n",
       " 'small_wiki/KING-FM - Wikipedia.html',\n",
       " 'small_wiki/The News-Times - Wikipedia.html',\n",
       " 'small_wiki/Robert Mercer (businessman) - Wikipedia.html',\n",
       " 'small_wiki/Federal Bureau of Investigation - Wikipedia.html',\n",
       " 'small_wiki/Viacom - Wikipedia.html',\n",
       " 'small_wiki/Seattle Municipal Street Railway - Wikipedia.html',\n",
       " 'small_wiki/Mandalay Bay - Wikipedia.html',\n",
       " 'small_wiki/Hal Abelson - Wikipedia.html',\n",
       " 'small_wiki/File_Googleplex-Patio-Aug-2014.JPG - Wikipedia.html',\n",
       " 'small_wiki/IEEE Computer Society - Wikipedia.html',\n",
       " 'small_wiki/Nintendo - Wikipedia.html',\n",
       " 'small_wiki/Mountain View, California - Wikipedia.html',\n",
       " 'small_wiki/University District, Seattle - Wikipedia.html',\n",
       " 'small_wiki/Category_Wikipedia articles with ISNI identifiers - Wikipedia.html',\n",
       " 'small_wiki/RealNetworks - Wikipedia.html',\n",
       " 'small_wiki/Robot (dance) - Wikipedia.html',\n",
       " 'small_wiki/Category_American Presbyterians - Wikipedia.html',\n",
       " 'small_wiki/Category_Wikipedia articles with NLA identifiers - Wikipedia.html',\n",
       " 'small_wiki/Henry Holt and Company - Wikipedia.html',\n",
       " 'small_wiki/David Swinson Maynard - Wikipedia.html',\n",
       " 'small_wiki/Hans-Hermann Hoppe - Wikipedia.html',\n",
       " 'small_wiki/Paul Gottfried - Wikipedia.html',\n",
       " 'small_wiki/MJ &amp; Friends - Wikipedia.html',\n",
       " 'small_wiki/Mesoparapylocheles - Wikipedia.html',\n",
       " 'small_wiki/Ubisoft - Wikipedia.html',\n",
       " 'small_wiki/Cultural impact of Michael Jackson - Wikipedia.html',\n",
       " 'small_wiki/AIM (software) - Wikipedia.html',\n",
       " 'small_wiki/Moonwalk (dance) - Wikipedia.html',\n",
       " 'small_wiki/Seattle Mardi Gras riot - Wikipedia.html',\n",
       " 'small_wiki/Black or White - Wikipedia.html',\n",
       " 'small_wiki/Unity Tour - Wikipedia.html',\n",
       " 'small_wiki/Baltimore Afro-American - Wikipedia.html',\n",
       " 'small_wiki/Thriller 25_ Limited Japanese Single Collection - Wikipedia.html',\n",
       " 'small_wiki/ABC (The Jackson 5 album) - Wikipedia.html',\n",
       " 'small_wiki/Republican Study Committee - Wikipedia.html',\n",
       " 'small_wiki/The Rolling Stone Album Guide - Wikipedia.html',\n",
       " 'small_wiki/Euro - Wikipedia.html',\n",
       " 'small_wiki/Living with Michael Jackson - Wikipedia.html',\n",
       " 'small_wiki/Republican Party of Virginia - Wikipedia.html',\n",
       " 'small_wiki/Humberto Gatica - Wikipedia.html',\n",
       " 'small_wiki/1964 Republican National Convention - Wikipedia.html',\n",
       " 'small_wiki/William E. Miller - Wikipedia.html',\n",
       " 'small_wiki/Light rail - Wikipedia.html',\n",
       " 'small_wiki/Fisheries science - Wikipedia.html',\n",
       " 'small_wiki/Steve Forbes - Wikipedia.html',\n",
       " 'small_wiki/In the Closet - Wikipedia.html',\n",
       " 'small_wiki/Andrew Breitbart - Wikipedia.html',\n",
       " 'small_wiki/Tenor - Wikipedia.html',\n",
       " 'small_wiki/Bob Corker - Wikipedia.html',\n",
       " 'small_wiki/Rhinoplasty - Wikipedia.html',\n",
       " 'small_wiki/VH1 - Wikipedia.html',\n",
       " \"small_wiki/Jehovah's Witnesses - Wikipedia.html\",\n",
       " 'small_wiki/Airliner - Wikipedia.html',\n",
       " 'small_wiki/List of songs recorded by The Jackson 5 - Wikipedia.html',\n",
       " 'small_wiki/Traditionalist conservatism - Wikipedia.html',\n",
       " 'small_wiki/Orrin Hatch - Wikipedia.html',\n",
       " 'small_wiki/Jeb Bush - Wikipedia.html',\n",
       " 'small_wiki/William McKinley - Wikipedia.html',\n",
       " 'small_wiki/Alcoholic drink - Wikipedia.html',\n",
       " 'small_wiki/Game of Thrones - Wikipedia.html',\n",
       " 'small_wiki/Michael Jackson - Wikipedia.html']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SearchEngine(\"small_wiki\", \".html\").search(\"data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
